
### 概率论回顾

2. 极大似然估计：假设样本x服从某种参数为θ的分布p。根据已知x，反推最大可能导致样本结果的参数θ。 这个结果，和`最小二乘（正规方程组）`计算出来的一样。
3. 为什么不用后者方法，直接算？因为数据量很大，直接算太慢。


>白化(whitening)
1. 目的:降低输入的冗余性。
2. 白化之后，学习算法的输入具有如下性质：
(i)特征之间相关性较低；(ii)所有特征具有相同的方差。


### NICE
> [链接](https://zhuanlan.zhihu.com/p/41912710)
1. h = f(x)：f是可逆的，h的维数与x的维数相同。
2. f是学习来的。先验分布pH(h)可以是预定义，也可以学习得来。
3. 表示学习：扩展与输入中“有趣”区域相关联的表示空间的体积。
4. 评估标准：**最大化** `log-likelihood`

加性耦合层：通过交错的方式来混合信息流（等价于直接反转原来的向量）

- 由于z的每个维度的独立性，理论上我们控制改变单个维度时，就可以看出生成图像是如何随着该维度的改变而改变，从而发现该维度的含义。
- 对两幅图像的编码进行插值（加权平均），得到过渡自然的生成样本，这些在后面发展起来的Glow模型中体现得很充分。

### RealNVP
> [链接](https://zhuanlan.zhihu.com/p/43048337)
1. 评估标准：`bits per dimension`-->`average negative log-likelihood`

加性和乘性耦合层结合 “仿射耦合层”
通过随机打乱（每一步 flow 输出的两个向量 h1,h2 拼接成一个向量 h，然后将这个向量重新随机排序），信息混合更充分，最终的 loss 可以更低。
分割和打乱操作，都只对“通道”轴执行。
在耦合模型中引入了卷积层

多尺度：
抛弃了 p(z) 是标准正态分布的直接假设，而采用了一个组合式的条件分布。
减少运算量，解决维度浪费问题

### Glow
[ZhiHu](https://zhuanlan.zhihu.com/p/39676312)
[openai-blog](https://openai.com/blog/glow/)
引入了`可逆1x1卷积`来代替排序层

h = xW
1. 初始化时，为了保证 W 的可逆性，使用"随机正交矩阵"初始化W。
2. W = PLU。将W进行LU分解。其中 P 是一个置换矩阵，也就是前面说的 shuffle 的等价矩阵；L 是一个下三角阵，对角线元素全为 1；U 是一个上三角阵。
3. 训练策略：固定 P，也固定 U 的对角线的正负号，然后约束 L 为对角线全 1 的下三角阵，U 为上三角阵，优化训练 L,U 的其余参数。 
4. 大家都训练到最优的情况下，可逆1×1卷积的图像生成质量才是最优的。（和简单反转、shuffle相比）但是，可逆1x1卷积达到饱和所需要的 epoch 数更多。



### Glow的优缺点
- 优点
对潜在变量插值，可以生成中间过渡类型的自然图像。（假设先验分布z的每个维度iid--> x的每个维度iid）
- 缺点：耗费资源太大
cifar10（32x32），跑了 700 个 epoch，效果：远看似乎还可以，近看啥都不是。
256x256高清人脸图像生成，需要训练 4000 个 epoch，用 40 个 GPU 训练了一周，简单理解就是用 1 个 GPU 训练一年……



### IDF
[IDF-code](https://github.com/jornpeters/integer_discrete_flows)