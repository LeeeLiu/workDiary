
### NEURALOGRAM: A NEURAL NETWORK BASED REPRESENTATION FOR UNDERSTANDING AUDIO SIGNALS
motivation：如今，音频理解、音乐推荐、元数据提取等方面有许多应用，所以怎样表示任意长的音频信号中有意义的关系很重要。但是，这些信号不容易用现有的算法来表示。
方法：提出了一种基于神经网络嵌入的音频信号表示方法。即将原始音频信号转换为密集、紧凑的表示（基于音高、音色和节奏，来表示音频其他属性）。由于嵌入索引（embedding index）与音频属性不是对应关系，所以这种方法类似于自然语言处理中的word2vec。
基于STFT的频谱表示Xspec，Xspec的变体是T(Xspec)。网络学习的是转换T。
学习到了与频谱正交的特征。
频率内容、音色、嵌入大小（太小了导致特征难以分辨）、节奏、语义

### Imperceptible, Robust and Targeted Adversarial Examples for Automatic Speech Recognition
[项目主页](http://cseweb.ucsd.edu/~yaq007/imperceptible-robust-adv.html)
[实现](https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_asr)

### SUBSPECTRALNET
[实现1](https://github.com/ssrp/SubSpectralNet-PyTorch)
[实现2](https://github.com/ssrp/SubSpectralNet)
[数据集](https://zenodo.org/record/1228142)


Q：
200×500 40×500
confusion matrices

目标：利用子声谱图，CNN网络来做声学场景分类：语义标签（商场、机场、街道）
方法：根据频带特性，将声谱图切分为子声谱图，放入子分类器。将子分类器的结果连接起来放入全局分类器。

分析：图像在空间的两个维度上都有相关性，而声谱图（两个维度分别是时间、频率）只有在时间维度上有相关性，频率上没有。

本文要提取的特征是log mel-spectrograms(梅尔声谱图)。特征尺寸为200×500(Mel-bins×Time-index)的样本。
一共有10个声学场景类别。在每个类别的样本中，将时间维度上的所有样本连接起来并取平均值.现在，我们得到了10个（200×1）的向量。分类方法有：
1. 为了更清晰起见，我们使用10个200维参考均值向量对测试样本进行了bin-wise分类。对于某个测试样本在时间方向取平均值得到200d向量，计算与参考均值向量的L2距离，距离最小的类别就是测试样本的类别。
2. 为了研究频率维度（即每个Mel-bin）对分类结果的影响，我们计算测试样本的200d向量与10个200d参考向量中对应mel-bin分量的距离（不是整体距离），距离最小的mel-bin分量所在的类别，即为测试样本的类别。如果分类正确，就算作对fig1（每个类别的mel-bin激活直方图）中对应类别对应mel-bin的activations直方图的贡献。
3. 为了研究每个类别的分布的相似程度，计算fig1中每两个类别之间的卡方距离，经过公式（1），得到fig2。（验证实验结果是否准确）

使用小些的卷积核更好，便于掌握局部时频信息。

局限性：
1. 对于某些分类，子分类器表现比全局分类器更好。
2. 子声谱图的size hop 需要人为地指定，如果能够有统计学方法计算出来就好了。
3. 未来需要对时间信息有效地建模。